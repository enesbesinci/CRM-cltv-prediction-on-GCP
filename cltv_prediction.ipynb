{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de270ad-3e6a-49a5-a37d-4561f7250122",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.cloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mydata_profiling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bigquery\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.cloud'"
     ]
    }
   ],
   "source": [
    "# first of all we need to import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abddf80-afaf-48e5-b232-f53e092108c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Then we create a BigQuery client object to pull the data into the workbench environment, and just like in the BigQuery environment, we pull our data with SQL commands and bring them into a dataframe format.\n",
    "client = bigquery.Client()\n",
    "\n",
    "raw_data = client.query(\"SELECT * FROM `bqml-cltv.cltv_dataset.flo_data_20k`\").to_dataframe()\n",
    "df = raw_data.copy()\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff68a7b-db50-4875-8a1b-b3de90eed229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we have about 20.000 row and 12 feature \n",
    "print(\"shape of the original data\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ded7de-c27b-4d83-b608-d9f932ed3a76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we received the data from BigQuery, so the date columns seem to be of the wrong type, we have to fix them.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff6190-b30f-4d6e-8014-67d79d724c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get column names as a list\n",
    "date_columns = list([\"first_order_date\",\"last_order_date\",\"last_order_date_online\",\"last_order_date_offline\"])\n",
    "\n",
    "for column in date_columns:\n",
    "    df[column] = pd.to_datetime(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45dac3-e110-437d-95f4-71bb28b1445b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# profiling\n",
    "profile = ProfileReport(df, title=\"FLO Profiling Report\")\n",
    "profile.to_file(\"FLO_Profiling_Report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3978b4d1-657f-4234-ae57-7ee3a3c58e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in order to observe the outlier values in the columns containing the number of orders and order amounts in both online and offline channels, let's simply visualize these columns with the boxplot.\n",
    "order_columns = [\"order_num_total_ever_online\",\"order_num_total_ever_offline\"]\n",
    "value_columns = [\"customer_value_total_ever_online\",\"customer_value_total_ever_offline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece833e-298a-4b1b-b9fc-a0199920eebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in order_columns:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    sns.boxplot(df[x], orient=\"h\")\n",
    "    plt.xlabel(x)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa59e1c9-89ff-461f-957c-9cd2e313a89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in value_columns:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    sns.boxplot(df[x], orient=\"h\")\n",
    "    plt.xlabel(x)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a2f1e-632d-4d07-a18b-a7d171423557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's write a simple function to see the missing values, this function will return the number of missing values and percentage of missing values.\n",
    "def missing_data_report(data):\n",
    "    # calculate the number of missing values\n",
    "    missing_values = data.isnull().sum()\n",
    "\n",
    "    # calculate the percentage of the missing values\n",
    "    percent_missing = (missing_values / data.shape[0]) * 100\n",
    "\n",
    "    # merge the results into a DataFrame.\n",
    "    result = pd.DataFrame({\n",
    "        \"number of missing values\": missing_values,\n",
    "        \"percentage of missing values\": percent_missing\n",
    "    })\n",
    "\n",
    "    # sort the results by number of missing values\n",
    "    result = result.sort_values(by=\"number of missing values\", ascending=False)\n",
    "\n",
    "    return result\n",
    "\n",
    "missing_data_report(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd64d1-e539-4bcb-b6d7-db161cd89535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As you can see we have no missing values, this is a good news because each observation is important for us, now we will define the numeric variables and save it as a csv file.\n",
    "df.describe().T.to_csv(\"summary_of_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b2ce7-fde1-47e5-82c3-d6a3ca784bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There are different methods to identify outliers, in this project I will use the IQR method. Let's see these outliers, I have written a simple function to see outliers, in this function we need to write the values Q1 and Q3.\n",
    "def detect_outliers(data, q1 = 0.25, q3 = 0.75, noe = 5):\n",
    "    outliers = {}\n",
    "    for column in data.columns:\n",
    "        if pd.api.types.is_numeric_dtype(data[column]):\n",
    "            Q1 = data[column].quantile(q1)\n",
    "            Q3 = data[column].quantile(q3)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            outlier_indices = (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    "            outliers[column] = data[column][outlier_indices]\n",
    "            \n",
    "    return pd.DataFrame(outliers).head(noe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5908557-d9f0-4ae1-b4eb-5b2aec9d01d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detect_outliers(df, q1 = 0.01, q3 = 0.99, noe = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315440a-3b0f-4bb4-abb5-d73b84fd089e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now that we have seen our outliers, we can either discard these outliers from the dataset or suppress them according to the values we want, we said that each observation is important, so we will suppress the observations that contain these outliers.\n",
    "def outlier_thresholds(data, variable, q1 = 0.25, q3 = 0.75):\n",
    "    Q1 = data[variable].quantile(q1)\n",
    "    Q3 = data[variable].quantile(q3)\n",
    "    interquantile_range = Q3 - Q1\n",
    "    up_limit = Q1 + 1.5 * interquantile_range\n",
    "    low_limit = Q3 - 1.5 * interquantile_range\n",
    "    return low_limit, up_limit\n",
    "\n",
    " \n",
    "def replace_with_thresholds(data, variable, q1 = 0.25, q3 = 0.75):\n",
    "    low_limit, up_limit = outlier_thresholds(data, variable, q1, q3)\n",
    "    data.loc[(data[variable] < low_limit), variable] = low_limit.round()\n",
    "    data.loc[(data[variable] > up_limit), variable] = up_limit.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97adcd29-2eee-4256-8d7a-6541b4befa78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a11253-afec-4e66-8f7a-0dfb4310ffb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "replace_with_thresholds(clean_data, \"order_num_total_ever_online\", q1 = 0.01, q3 = 0.99)\n",
    "replace_with_thresholds(clean_data, \"order_num_total_ever_offline\", q1 = 0.01, q3 = 0.99)\n",
    "replace_with_thresholds(clean_data, \"customer_value_total_ever_offline\", q1 = 0.01, q3 = 0.99)\n",
    "replace_with_thresholds(clean_data, \"customer_value_total_ever_online\", q1 = 0.01, q3 = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9c7be-5c9f-4a34-84eb-0d2ed65eeade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detect_outliers(clean_data, q1 = 0.01, q3 = 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338fae5c-550c-43e0-9fbb-e9b7d5a7f187",
   "metadata": {},
   "source": [
    "# creating metrics to calculate CLTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c37f62-e94f-4747-87ec-d2e31b7eace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Vertex Ai Workbench for these calculations because of the high computing power capacity offered by Google. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b6a97-bbb3-4bba-a275-377924470ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new more simple dataframe to calculate the cltv easily\n",
    "clv_data = clean_data[[\"master_id\",\n",
    "                      \"first_order_date\",\n",
    "                      \"last_order_date\",\n",
    "                      \"order_num_total_ever_online\",\n",
    "                      \"order_num_total_ever_offline\",\n",
    "                      \"customer_value_total_ever_offline\",\n",
    "                      \"customer_value_total_ever_online\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f4f23-2531-4029-80ab-61d3196a6bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_data.to_csv(\"clean_data_for_cltv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe66f5f-675a-4b60-a691-296edc03d858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We set the analysis date as 2 days after the last date in the dataset for this demo.\n",
    "last_day_on_data = clv_data.last_order_date.max()\n",
    "analysis_day = last_day_on_data + dt.timedelta(days=2)\n",
    "\n",
    "a_week = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79096f5a-669d-451d-b601-c29903ec0639",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analysis_day' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# tenure hesaplanması\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m clv_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT_weekly\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43manalysis_day\u001b[49m \u001b[38;5;241m-\u001b[39m clv_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_order_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m clv_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT_weekly\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ((clv_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT_weekly\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m a_week)\u001b[38;5;241m.\u001b[39mround()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'analysis_day' is not defined"
     ]
    }
   ],
   "source": [
    "# Tenure = Analysis day - Customer's first order date / 7. Tenure metric should be weekly.\n",
    "\n",
    "clv_data[\"T_weekly\"] = analysis_day - clv_data[\"first_order_date\"]\n",
    "clv_data[\"T_weekly\"] = ((clv_data[\"T_weekly\"] // pd.Timedelta(days=1)) / a_week).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb14f4-6c2b-4450-a595-ae0b0c293846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recency = Customer's last order date - Customer's first order date / 7. Recency value should be daily, weekly or yearly but we get it weekly.\n",
    "\n",
    "clv_data[\"recency_weekly\"] = clv_data[\"last_order_date\"] - clv_data[\"first_order_date\"]\n",
    "clv_data[\"recency_weekly\"] = ((clv_data[\"recency_weekly\"] // pd.Timedelta(days=1)) / a_week).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33876b6-064b-48cb-94f0-ea41c3d92bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency = Total number of transaction. Minimum frequency should be 2 to calculate the CLTV.\n",
    "\n",
    "clv_data[\"frequency\"] = (clv_data[\"order_num_total_ever_online\"] + clv_data[\"order_num_total_ever_offline\"]).round()\n",
    "clv_data = clv_data[clv_data[\"frequency\"] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7600b2c-29b8-49d0-9039-fe389c590b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monetary = Total purchase amount / Total number of transaction.\n",
    "\n",
    "clv_data[\"monetary_avg\"] = ((clv_data[\"customer_value_total_ever_offline\"] + clv_data[\"customer_value_total_ever_online\"]) / clv_data[\"frequency\"]).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e732ddf-8de7-4ba7-9c7c-b6af928b8866",
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db67d67-1b81-400f-84b6-cde5f09757fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bg/nbd model\n",
    "from lifetimes import BetaGeoFitter\n",
    "\n",
    "# fit the model\n",
    "bgf = BetaGeoFitter(penalizer_coef=0.001)\n",
    "\n",
    "bgf.fit(clv_data[\"frequency\"],\n",
    "        clv_data[\"recency_weekly\"],\n",
    "        clv_data[\"T_weekly\"])\n",
    "\n",
    "# predict for 3 months\n",
    "three_month = 4 * 3\n",
    "clv_data[\"exp_sales_3_month\"] = bgf.conditional_expected_number_of_purchases_up_to_time(three_month,\n",
    "                                                                                        clv_data[\"frequency\"],\n",
    "                                                                                        clv_data[\"recency_weekly\"],\n",
    "                                                                                        clv_data[\"T_weekly\"])\n",
    "\n",
    "# predict for 6 months\n",
    "six_months = 4 * 6\n",
    "clv_data[\"exp_sales_6_month\"] = bgf.conditional_expected_number_of_purchases_up_to_time(six_months,\n",
    "                                                                                        clv_data[\"frequency\"],\n",
    "                                                                                        clv_data[\"recency_weekly\"],\n",
    "                                                                                        clv_data[\"T_weekly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bab720-1a7e-434d-9b26-1dec9528cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma gamma model\n",
    "from lifetimes import GammaGammaFitter\n",
    "\n",
    "# fit the model\n",
    "ggf = GammaGammaFitter(penalizer_coef=0.01)\n",
    "ggf.fit(clv_data['frequency'], clv_data['monetary_avg'])\n",
    "\n",
    "# get predicts\n",
    "clv_data[\"exp_average_value\"] = ggf.conditional_expected_average_profit(clv_data['frequency'],\n",
    "                                                                       clv_data['monetary_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e025aa-6abc-4a7e-a2f8-f5ec187b344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the customer life time value\n",
    "\n",
    "clv_data[\"clv\"] = ggf.customer_lifetime_value(bgf,\n",
    "                                   clv_data['frequency'],\n",
    "                                   clv_data['recency_weekly'],\n",
    "                                   clv_data['T_weekly'],\n",
    "                                   clv_data['monetary_avg'],\n",
    "                                   time=6,  # 6 months\n",
    "                                   freq=\"W\",  # W = Week\n",
    "                                   discount_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06050e-c738-4c68-92e2-e7ddddc2a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_data[\"customer_segment\"] = pd.qcut(clv_data[\"clv\"], 4, labels=[\"D\", \"C\", \"B\", \"A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363e045-b6be-429b-ace2-3afe38c885eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625f447-5a33-4851-b825-986f52a1d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_data.groupby(\"customer_segment\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4d525-5652-4b96-b2cd-4ee4b50b7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_A_customers = clv_data[clv_data[\"customer_segment\"] == \"A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76cc555-baf2-4c45-88f7-5cda59bcfed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_A_customers.to_csv(\"segment_A_customer.csv\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
